{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "\n",
    "    def __init__(self, in_features, out_feature_list, b_dim, dropout):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_feature_list = out_feature_list\n",
    "\n",
    "        self.linear1 = nn.Linear(in_features, out_feature_list[0])\n",
    "        self.linear2 = nn.Linear(out_feature_list[0], out_feature_list[1])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, adj, activation=None):\n",
    "        # input : 16x9x9\n",
    "        # adj : 16x4x9x9\n",
    "\n",
    "        hidden = torch.stack([self.linear1(input) for _ in range(adj.size(1))], 1)\n",
    "        hidden = torch.einsum('bijk,bikl->bijl', (adj, hidden))\n",
    "        hidden = torch.sum(hidden, 1) + self.linear1(input)\n",
    "        hidden = activation(hidden) if activation is not None else hidden\n",
    "        hidden = self.dropout(hidden)\n",
    "\n",
    "        output = torch.stack([self.linear2(hidden) for _ in range(adj.size(1))], 1)\n",
    "        output = torch.einsum('bijk,bikl->bijl', (adj, output))\n",
    "        output = torch.sum(output, 1) + self.linear2(hidden)\n",
    "        output = activation(output) if activation is not None else output\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAggregation(Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, b_dim, dropout):\n",
    "        super(GraphAggregation, self).__init__()\n",
    "        self.sigmoid_linear = nn.Sequential(nn.Linear(in_features+b_dim, out_features),\n",
    "                                            nn.Sigmoid())\n",
    "        self.tanh_linear = nn.Sequential(nn.Linear(in_features+b_dim, out_features),\n",
    "                                         nn.Tanh())\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, activation):\n",
    "        i = self.sigmoid_linear(input)\n",
    "        j = self.tanh_linear(input)\n",
    "        output = torch.sum(torch.mul(i,j), 1)\n",
    "        output = activation(output) if activation is not None\\\n",
    "                 else output\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
